{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-LLC/FinRL-Library/blob/master/FinRL_single_stock_trading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Idd1jem0TnST"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Single Stock Trading\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade single stock in one Jupyter Notebook | Presented at NeurIPS 2020: Deep RL Workshop\n",
    "\n",
    "* This blog is based on our paper: FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance, presented at NeurIPS 2020: Deep RL Workshop.\n",
    "* Check out medium blog for detailed explanations: https://towardsdatascience.com/finrl-for-quantitative-finance-tutorial-for-single-stock-trading-37d6d7c30aac\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vglc_9N5-KZ"
   },
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ex2ord116AbP"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1eLhMW36cLi"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eimeRv06YoK"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: single stock trading for AAPL\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n",
    "\n",
    "We use Apple Inc. stock: AAPL as an example throughout this article, because it is one of the most popular and profitable stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD3f90UnTnSU"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBUcBKap-oII"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40axJBAP5mer",
    "outputId": "0e61993b-e962-4b70-a178-fe86404dd1ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/AI4Finance-LLC/FinRL-Library.git\n",
      "  Cloning https://github.com/AI4Finance-LLC/FinRL-Library.git to /private/var/folders/v8/qfm3c47x1flgbmpw1c9k7s500000gn/T/pip-req-build-mbkhzn6p\n",
      "  Running command git clone -q https://github.com/AI4Finance-LLC/FinRL-Library.git /private/var/folders/v8/qfm3c47x1flgbmpw1c9k7s500000gn/T/pip-req-build-mbkhzn6p\n",
      "Requirement already satisfied (use --upgrade to upgrade): finrl==0.0.3 from git+https://github.com/AI4Finance-LLC/FinRL-Library.git in /Users/Nick/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (1.17.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (1.1.5)\n",
      "Requirement already satisfied: stockstats in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (0.3.2)\n",
      "Requirement already satisfied: yfinance in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (0.1.55)\n",
      "Requirement already satisfied: matplotlib in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (0.21.3)\n",
      "Requirement already satisfied: gym>=0.17 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (0.18.0)\n",
      "Requirement already satisfied: stable-baselines3[extra] in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (0.10.0)\n",
      "Requirement already satisfied: pytest in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (5.2.1)\n",
      "Requirement already satisfied: setuptools>=41.4.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (41.6.0)\n",
      "Requirement already satisfied: wheel>=0.33.6 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (0.33.6)\n",
      "Requirement already satisfied: pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2 from git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from finrl==0.0.3) (0.9.2+75.g4b901f6)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pandas>=1.1.5->finrl==0.0.3) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pandas>=1.1.5->finrl==0.0.3) (2.8.0)\n",
      "Requirement already satisfied: int-date>=0.1.7 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from stockstats->finrl==0.0.3) (0.1.8)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from yfinance->finrl==0.0.3) (2.22.0)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from yfinance->finrl==0.0.3) (0.0.9)\n",
      "Requirement already satisfied: lxml>=4.5.1 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from yfinance->finrl==0.0.3) (4.6.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from matplotlib->finrl==0.0.3) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from matplotlib->finrl==0.0.3) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from matplotlib->finrl==0.0.3) (2.4.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from scikit-learn>=0.21.0->finrl==0.0.3) (1.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from scikit-learn>=0.21.0->finrl==0.0.3) (0.14.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from gym>=0.17->finrl==0.0.3) (6.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from gym>=0.17->finrl==0.0.3) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from gym>=0.17->finrl==0.0.3) (1.2.2)\n",
      "Requirement already satisfied: torch>=1.4.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from stable-baselines3[extra]->finrl==0.0.3) (1.7.1)\n",
      "Requirement already satisfied: opencv-python; extra == \"extra\" in /Users/Nick/anaconda3/lib/python3.6/site-packages (from stable-baselines3[extra]->finrl==0.0.3) (4.5.1.48)\n",
      "Requirement already satisfied: atari-py~=0.2.0; extra == \"extra\" in /Users/Nick/anaconda3/lib/python3.6/site-packages (from stable-baselines3[extra]->finrl==0.0.3) (0.2.6)\n",
      "Requirement already satisfied: psutil; extra == \"extra\" in /Users/Nick/anaconda3/lib/python3.6/site-packages (from stable-baselines3[extra]->finrl==0.0.3) (5.6.3)\n",
      "Requirement already satisfied: tensorboard; extra == \"extra\" in /Users/Nick/anaconda3/lib/python3.6/site-packages (from stable-baselines3[extra]->finrl==0.0.3) (2.0.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pytest->finrl==0.0.3) (1.8.0)\n",
      "Requirement already satisfied: packaging in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pytest->finrl==0.0.3) (19.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pytest->finrl==0.0.3) (19.2.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pytest->finrl==0.0.3) (7.2.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pytest->finrl==0.0.3) (1.3.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pytest->finrl==0.0.3) (0.13.0)\n",
      "Requirement already satisfied: wcwidth in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pytest->finrl==0.0.3) (0.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pytest->finrl==0.0.3) (0.23)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.9.0)\n",
      "Requirement already satisfied: empyrical>=0.5.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.5.5)\n",
      "Requirement already satisfied: ipython>=3.2.3 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (7.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas>=1.1.5->finrl==0.0.3) (1.12.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from requests>=2.20->yfinance->finrl==0.0.3) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from requests>=2.20->yfinance->finrl==0.0.3) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from requests>=2.20->yfinance->finrl==0.0.3) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from requests>=2.20->yfinance->finrl==0.0.3) (3.0.4)\n",
      "Requirement already satisfied: future in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17->finrl==0.0.3) (0.18.2)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /Users/Nick/anaconda3/lib/python3.6/site-packages (from torch>=1.4.0->stable-baselines3[extra]->finrl==0.0.3) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /Users/Nick/anaconda3/lib/python3.6/site-packages (from torch>=1.4.0->stable-baselines3[extra]->finrl==0.0.3) (3.7.4.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]->finrl==0.0.3) (3.1.1)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]->finrl==0.0.3) (1.24.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]->finrl==0.0.3) (0.16.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]->finrl==0.0.3) (3.10.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from tensorboard; extra == \"extra\"->stable-baselines3[extra]->finrl==0.0.3) (0.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from importlib-metadata>=0.12->pytest->finrl==0.0.3) (0.6.0)\n",
      "Requirement already satisfied: pandas-datareader>=0.2 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from empyrical>=0.5.0->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.9.0)\n",
      "Requirement already satisfied: decorator in /Users/Nick/anaconda3/lib/python3.6/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (4.4.0)\n",
      "Requirement already satisfied: pickleshare in /Users/Nick/anaconda3/lib/python3.6/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (2.0.10)\n",
      "Requirement already satisfied: pygments in /Users/Nick/anaconda3/lib/python3.6/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (2.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (4.3.3)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.15.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/Nick/anaconda3/lib/python3.6/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (4.7.0)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/Nick/anaconda3/lib/python3.6/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.1.0)\n",
      "Requirement already satisfied: backcall in /Users/Nick/anaconda3/lib/python3.6/site-packages (from ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.1.0)\n",
      "Requirement already satisfied: ipython-genutils in /Users/Nick/anaconda3/lib/python3.6/site-packages (from traitlets>=4.2->ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.2.0)\n",
      "Requirement already satisfied: parso>=0.5.0 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from jedi>=0.10->ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.5.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/Nick/anaconda3/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=3.2.3->pyfolio@ git+https://github.com/quantopian/pyfolio.git#egg=pyfolio-0.9.2->finrl==0.0.3) (0.6.0)\n",
      "Building wheels for collected packages: finrl\n",
      "  Building wheel for finrl (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for finrl: filename=finrl-0.0.3-cp36-none-any.whl size=28336 sha256=ba72612dde3b0f08748648a3c0ed1d039e90b4adf155af425b477891a168e76e\n",
      "  Stored in directory: /private/var/folders/v8/qfm3c47x1flgbmpw1c9k7s500000gn/T/pip-ephem-wheel-cache-v5219jf7/wheels/9c/19/bf/c644def96612df1ad42c94d5304966797eaa3221dffc5efe0b\n",
      "Successfully built finrl\n"
     ]
    }
   ],
   "source": [
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-LLC/FinRL-Library.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXyHD6ir5sxk"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QG17M4JwTnSZ"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-0bsNMMTnSZ",
    "outputId": "e1c336ac-1280-4b16-b1d4-c2971d6667c2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "from finrl.config import config\n",
    "from finrl.marketdata.yahoodownloader import YahooDownloader\n",
    "from finrl.preprocessing.preprocessors import FeatureEngineer\n",
    "from finrl.preprocessing.data import data_split\n",
    "from finrl.env.env_stocktrading import StockTradingEnv\n",
    "from finrl.model.models import DRLAgent\n",
    "from finrl.trade.backtest import backtest_stats, get_baseline, backtest_plot\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uIPbzYs1TnSd"
   },
   "outputs": [],
   "source": [
    "#Diable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pos2IZAL54pp"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zp6lL6dZ53rX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
    "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
    "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
    "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
    "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
    "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
    "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"./\" + config.RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBPM0sVvTnSg"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCGVmtGzjORf"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FBEiH5gOgMOx",
    "outputId": "de388576-0110-46f4-9257-dfe327b3eac5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000-01-01'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from config.py start_date is a string\n",
    "config.START_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sWLMNQ8CgMRx",
    "outputId": "f1864c8b-2b5c-4867-9122-ccf83ebc2902"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-01-01'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from config.py end_date is a string\n",
    "config.END_DATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmFpuBEZhkF3"
   },
   "source": [
    "ticker_list is a list of stock tickers, in a single stock trading case, the list contains only 1 ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtIFikNyTnSg",
    "outputId": "4c6a860f-6944-4728-984d-d0dc8d8f2c78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Shape of DataFrame:  (3021, 8)\n"
     ]
    }
   ],
   "source": [
    "# Download and save the data in a pandas DataFrame:\n",
    "data_df = YahooDownloader(start_date = '2009-01-01',\n",
    "                          end_date = '2021-01-01',\n",
    "                          ticker_list = ['AAPL']).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8ZKQmw6TnSl",
    "outputId": "954cbef4-f75b-4d4b-80f2-195eb13954ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3021, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "Vi6D_ED6TnSs",
    "outputId": "2313cd51-596f-4a58-922b-5ec837bbcbda"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>3.067143</td>\n",
       "      <td>3.251429</td>\n",
       "      <td>3.041429</td>\n",
       "      <td>2.795913</td>\n",
       "      <td>746015200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>3.327500</td>\n",
       "      <td>3.435000</td>\n",
       "      <td>3.311071</td>\n",
       "      <td>2.913912</td>\n",
       "      <td>1181608400</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>3.426786</td>\n",
       "      <td>3.470357</td>\n",
       "      <td>3.299643</td>\n",
       "      <td>2.865849</td>\n",
       "      <td>1289310400</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>3.278929</td>\n",
       "      <td>3.303571</td>\n",
       "      <td>3.223572</td>\n",
       "      <td>2.803923</td>\n",
       "      <td>753048800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>3.229643</td>\n",
       "      <td>3.326786</td>\n",
       "      <td>3.215714</td>\n",
       "      <td>2.855991</td>\n",
       "      <td>673500800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      open      high       low     close      volume   tic  day\n",
       "0  2009-01-02  3.067143  3.251429  3.041429  2.795913   746015200  AAPL    4\n",
       "1  2009-01-05  3.327500  3.435000  3.311071  2.913912  1181608400  AAPL    0\n",
       "2  2009-01-06  3.426786  3.470357  3.299643  2.865849  1289310400  AAPL    1\n",
       "3  2009-01-07  3.278929  3.303571  3.223572  2.803923   753048800  AAPL    2\n",
       "4  2009-01-08  3.229643  3.326786  3.215714  2.855991   673500800  AAPL    3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWiqgpLzTnS3"
   },
   "source": [
    "<a id='3'></a>\n",
    "# Part 4. Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* FinRL uses a class **FeatureEngineer** to preprocess the data\n",
    "* Add **technical indicators**. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJ9zmxpRks41"
   },
   "source": [
    "class FeatureEngineer:\n",
    "Provides methods for preprocessing the stock price data\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        df: DataFrame\n",
    "            data downloaded from Yahoo API\n",
    "        feature_number : int\n",
    "            number of features we used\n",
    "        use_technical_indicator : boolean\n",
    "            we technical indicator or not\n",
    "        use_turbulence : boolean\n",
    "            use turbulence index or not\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    preprocess_data()\n",
    "        main method to do the feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHu7i-T_wRPc"
   },
   "source": [
    "<a id='3.1'></a>\n",
    "\n",
    "## 4.1 Technical Indicators\n",
    "* FinRL uses stockstats to calcualte technical indicators such as **Moving Average Convergence Divergence (MACD)**, **Relative Strength Index (RSI)**, **Average Directional Index (ADX)**, **Commodity Channel Index (CCI)** and other various indicators and stats.\n",
    "* **stockstats**: supplies a wrapper StockDataFrame based on the **pandas.DataFrame** with inline stock statistics/indicators support.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RHwY1dHk09N",
    "outputId": "47be0df8-9e74-473e-f78e-9f267bcce3df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30', 'close_30_sma', 'close_60_sma']\n"
     ]
    }
   ],
   "source": [
    "## we store the stockstats technical indicator column names in config.py\n",
    "tech_indicator_list=config.TECHNICAL_INDICATORS_LIST\n",
    "print(tech_indicator_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rImfGAfCkR8j",
    "outputId": "5b55c2c7-3217-49ad-95b7-d9af0ad91d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30', 'close_30_sma', 'close_60_sma', 'kdjk', 'open_2_sma', 'boll', 'close_10.0_le_5_c', 'wr_10', 'dma', 'trix']\n"
     ]
    }
   ],
   "source": [
    "## user can add more technical indicators\n",
    "## check https://github.com/jealous/stockstats for different names\n",
    "tech_indicator_list=tech_indicator_list+['kdjk','open_2_sma','boll','close_10.0_le_5_c','wr_10','dma','trix']\n",
    "print(tech_indicator_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etvRo2rSwZPg"
   },
   "source": [
    "<a id='3.2'></a>\n",
    "## 4.2 Perform Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAOsx0m-9u2k",
    "outputId": "6238a0f8-e0bb-4468-ec75-ab3381ea09a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = tech_indicator_list,\n",
    "                    use_turbulence=False,\n",
    "                    user_defined_feature = False)\n",
    "\n",
    "data_df = fe.preprocess_data(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "wytX_qwWMHP5",
    "outputId": "8113269b-39a0-462b-80b2-b8664eb27a66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>kdjk</th>\n",
       "      <th>open_2_sma</th>\n",
       "      <th>boll</th>\n",
       "      <th>close_10.0_le_5_c</th>\n",
       "      <th>wr_10</th>\n",
       "      <th>dma</th>\n",
       "      <th>trix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>3.067143</td>\n",
       "      <td>3.251429</td>\n",
       "      <td>3.041429</td>\n",
       "      <td>2.795913</td>\n",
       "      <td>746015200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.021788</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.795913</td>\n",
       "      <td>2.795913</td>\n",
       "      <td>-5.637387</td>\n",
       "      <td>3.067143</td>\n",
       "      <td>2.795913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>216.912162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.670734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>3.327500</td>\n",
       "      <td>3.435000</td>\n",
       "      <td>3.311071</td>\n",
       "      <td>2.913912</td>\n",
       "      <td>1181608400</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>3.021788</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.854912</td>\n",
       "      <td>2.854912</td>\n",
       "      <td>-14.558226</td>\n",
       "      <td>3.197322</td>\n",
       "      <td>2.854912</td>\n",
       "      <td>2.0</td>\n",
       "      <td>132.399904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.670734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>3.426786</td>\n",
       "      <td>3.470357</td>\n",
       "      <td>3.299643</td>\n",
       "      <td>2.865849</td>\n",
       "      <td>1289310400</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>2.977231</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>2.858558</td>\n",
       "      <td>2.858558</td>\n",
       "      <td>-23.350262</td>\n",
       "      <td>3.377143</td>\n",
       "      <td>2.858558</td>\n",
       "      <td>3.0</td>\n",
       "      <td>140.934334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>3.278929</td>\n",
       "      <td>3.303571</td>\n",
       "      <td>3.223572</td>\n",
       "      <td>2.803923</td>\n",
       "      <td>753048800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000747</td>\n",
       "      <td>2.956137</td>\n",
       "      <td>...</td>\n",
       "      <td>43.608349</td>\n",
       "      <td>2.844899</td>\n",
       "      <td>2.844899</td>\n",
       "      <td>-34.024085</td>\n",
       "      <td>3.352857</td>\n",
       "      <td>2.844899</td>\n",
       "      <td>4.0</td>\n",
       "      <td>155.371731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-08</td>\n",
       "      <td>3.229643</td>\n",
       "      <td>3.326786</td>\n",
       "      <td>3.215714</td>\n",
       "      <td>2.855991</td>\n",
       "      <td>673500800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>2.943962</td>\n",
       "      <td>...</td>\n",
       "      <td>48.358256</td>\n",
       "      <td>2.847118</td>\n",
       "      <td>2.847118</td>\n",
       "      <td>-37.093625</td>\n",
       "      <td>3.254286</td>\n",
       "      <td>2.847118</td>\n",
       "      <td>5.0</td>\n",
       "      <td>143.232705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      open      high       low     close      volume   tic  day  \\\n",
       "0  2009-01-02  3.067143  3.251429  3.041429  2.795913   746015200  AAPL    4   \n",
       "1  2009-01-05  3.327500  3.435000  3.311071  2.913912  1181608400  AAPL    0   \n",
       "2  2009-01-06  3.426786  3.470357  3.299643  2.865849  1289310400  AAPL    1   \n",
       "3  2009-01-07  3.278929  3.303571  3.223572  2.803923   753048800  AAPL    2   \n",
       "4  2009-01-08  3.229643  3.326786  3.215714  2.855991   673500800  AAPL    3   \n",
       "\n",
       "       macd   boll_ub  ...       dx_30  close_30_sma  close_60_sma       kdjk  \\\n",
       "0  0.000000  3.021788  ...  100.000000      2.795913      2.795913  -5.637387   \n",
       "1  0.002647  3.021788  ...  100.000000      2.854912      2.854912 -14.558226   \n",
       "2  0.001883  2.977231  ...  100.000000      2.858558      2.858558 -23.350262   \n",
       "3 -0.000747  2.956137  ...   43.608349      2.844899      2.844899 -34.024085   \n",
       "4 -0.000088  2.943962  ...   48.358256      2.847118      2.847118 -37.093625   \n",
       "\n",
       "   open_2_sma      boll  close_10.0_le_5_c       wr_10  dma      trix  \n",
       "0    3.067143  2.795913                1.0  216.912162  0.0  0.670734  \n",
       "1    3.197322  2.854912                2.0  132.399904  0.0  0.670734  \n",
       "2    3.377143  2.858558                3.0  140.934334  0.0  0.391303  \n",
       "3    3.352857  2.844899                4.0  155.371731  0.0  0.195391  \n",
       "4    3.254286  2.847118                5.0  143.232705  0.0  0.125124  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwLhXo1cTnTQ"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Build Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1D1FlBdOL4b3"
   },
   "source": [
    "<a id='4.1'></a>\n",
    "## 5.1 Training & Trade data split\n",
    "* Training: 2009-01-01 to 2018-12-31\n",
    "* Trade: 2019-01-01 to 2020-09-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xNOdqfTKL6K-"
   },
   "outputs": [],
   "source": [
    "#train = data_split(data_df, start = config.START_DATE, end = config.START_TRADE_DATE)\n",
    "#trade = data_split(data_df, start = config.START_TRADE_DATE, end = config.END_DATE)\n",
    "train = data_split(data_df, start = '2009-01-01', end = '2019-01-01')\n",
    "trade = data_split(data_df, start = '2019-01-01', end = '2021-01-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KUxOshVouLXt"
   },
   "outputs": [],
   "source": [
    "## data normalization, this part is optional, have little impact\n",
    "#feaures_list = list(train.columns)\n",
    "#feaures_list.remove('date')\n",
    "#feaures_list.remove('tic')\n",
    "#feaures_list.remove('close')\n",
    "#print(feaures_list)\n",
    "#from sklearn import preprocessing\n",
    "#data_normaliser = preprocessing.StandardScaler()\n",
    "#train[feaures_list] = data_normaliser.fit_transform(train[feaures_list])\n",
    "#trade[feaures_list] = data_normaliser.transform(trade[feaures_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMHyaSBBDGbe"
   },
   "source": [
    "<a id='4.2'></a>\n",
    "## 5.2 User-defined Environment: a simulation environment class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90V3S7cpDcQs"
   },
   "source": [
    "<a id='4.3'></a>\n",
    "## 5.3 Initialize Environment\n",
    "* **stock dimension**: the number of unique stock tickers we use\n",
    "* **hmax**: the maximum amount of shares to buy or sell\n",
    "* **initial amount**: the amount of money we use to trade in the begining\n",
    "* **transaction cost percentage**: a per share rate for every share trade\n",
    "* **tech_indicator_list**: a list of technical indicator names (modified from config.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiH0xO96mGcL",
    "outputId": "490190d9-80dd-4ba0-c15f-2143cdd8341f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macd',\n",
       " 'boll_ub',\n",
       " 'boll_lb',\n",
       " 'rsi_30',\n",
       " 'cci_30',\n",
       " 'dx_30',\n",
       " 'close_30_sma',\n",
       " 'close_60_sma',\n",
       " 'kdjk',\n",
       " 'open_2_sma',\n",
       " 'boll',\n",
       " 'close_10.0_le_5_c',\n",
       " 'wr_10',\n",
       " 'dma',\n",
       " 'trix']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we store the stockstats technical indicator column names in config.py\n",
    "## check https://github.com/jealous/stockstats for different names\n",
    "tech_indicator_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnWOKS7DyGM7",
    "outputId": "09dff84b-19e1-4c4a-c40f-d5280195eaf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the stock dimension is 1, because we only use the price data of AAPL.\n",
    "len(train.tic.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipJcnLvQGAba",
    "outputId": "dc32ebc6-2191-4541-99d2-7e9861da5df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 11\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(config.TECHNICAL_INDICATORS_LIST)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JY5wTwYjGTrg"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 100000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": config.TECHNICAL_INDICATORS_LIST, \n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4\n",
    "    \n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vmyi2IPnyEkP",
    "outputId": "a968a96a-a1ee-426e-eed8-ee74d20b9bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdnzYtM1TnTW"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "BRFIZDw8TnTX"
   },
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zD1NHzGyTnTc"
   },
   "source": [
    "### Model Training: 5 models, A2C DDPG, PPO, TD3, SAC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9CM5DeIr9GC"
   },
   "source": [
    "### Model 1: A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOk-Mr73-EEq",
    "outputId": "1c56a5c6-91ad-4b81-dd07-24ae69e64c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0002}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002}\n",
    "model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXHEidJh-E60",
    "outputId": "9ae1ede1-a8ee-4e56-c4f3-ef809e05d664"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 242       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.000402 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 8.97e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 246      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.48    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.00948 |\n",
      "|    std                | 1.06     |\n",
      "|    value_loss         | 9.55e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 254      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.0346  |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 0.000388 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 256      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.0027  |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 5.82e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 257      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.0445   |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 0.00092  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| environment/          |          |\n",
      "|    portfolio_value    | 1.04e+05 |\n",
      "|    total_cost         | 3.02e+03 |\n",
      "|    total_reward       | 3.68e+03 |\n",
      "|    total_reward_pct   | 3.68     |\n",
      "|    total_trades       | 2305     |\n",
      "| time/                 |          |\n",
      "|    fps                | 267      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | -3.3e+11 |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00176  |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 8.01e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 280      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.141    |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 292      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.0156   |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 0.000262 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 299      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.0187  |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 0.00026  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 306      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.25    |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 0.0752   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 8.93e+04  |\n",
      "|    total_cost         | 3.18e+03  |\n",
      "|    total_reward       | -1.07e+04 |\n",
      "|    total_reward_pct   | -10.7     |\n",
      "|    total_trades       | 2359      |\n",
      "| time/                 |           |\n",
      "|    fps                | 312       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 0.0769    |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 0.002     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 317      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.174   |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 0.0302   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 318      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | -3.42    |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.602    |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 0.328    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 322      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | -0.0469  |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.0516  |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 0.0171   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 327      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.227   |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 0.0863   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| environment/          |          |\n",
      "|    portfolio_value    | 2.65e+05 |\n",
      "|    total_cost         | 3.47e+03 |\n",
      "|    total_reward       | 1.65e+05 |\n",
      "|    total_reward_pct   | 165      |\n",
      "|    total_trades       | 2488     |\n",
      "| time/                 |          |\n",
      "|    fps                | 328      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.0294   |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 0.000389 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 329       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | -4.42e+11 |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.00247  |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 2.77e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 327      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.00218  |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 0.00191  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 326      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.0765  |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 0.00118  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 323      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.00562 |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 2.59e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| environment/          |          |\n",
      "|    portfolio_value    | 1.31e+05 |\n",
      "|    total_cost         | 3.39e+03 |\n",
      "|    total_reward       | 3.11e+04 |\n",
      "|    total_reward_pct   | 31.1     |\n",
      "|    total_trades       | 2430     |\n",
      "| time/                 |          |\n",
      "|    fps                | 324      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.00576  |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 2.16e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 325      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 0.00396  |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 5.29e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 326      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -0.00179 |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 2.02e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 328      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.00489  |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 1.26e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 330      |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 0.0091   |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 7.13e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.38e+04  |\n",
      "|    total_cost         | 2.85e+03  |\n",
      "|    total_reward       | -6.23e+03 |\n",
      "|    total_reward_pct   | -6.23     |\n",
      "|    total_trades       | 2136      |\n",
      "| time/                 |           |\n",
      "|    fps                | 331       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.62     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | -0.00167  |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 3.51e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 333      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 0.00868  |\n",
      "|    std                | 1.23     |\n",
      "|    value_loss         | 4.08e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -0.00157 |\n",
      "|    std                | 1.25     |\n",
      "|    value_loss         | 1.23e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -0.00202 |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 2.33e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 337      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.00263  |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 4.56e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.96e+04  |\n",
      "|    total_cost         | 2.16e+03  |\n",
      "|    total_reward       | -410      |\n",
      "|    total_reward_pct   | -0.41     |\n",
      "|    total_trades       | 1653      |\n",
      "| time/                 |           |\n",
      "|    fps                | 339       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | -0.000221 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 2.73e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 46       |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -0.00137 |\n",
      "|    std                | 1.31     |\n",
      "|    value_loss         | 7.26e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 334      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -0.0011  |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 2.44e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 334       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | -4.72e+10 |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 0.00349   |\n",
      "|    std                | 1.35      |\n",
      "|    value_loss         | 3.59e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 334      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 0.00219  |\n",
      "|    std                | 1.37     |\n",
      "|    value_loss         | 7.73e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.71e+04  |\n",
      "|    total_cost         | 1.74e+03  |\n",
      "|    total_reward       | -2.94e+03 |\n",
      "|    total_reward_pct   | -2.94     |\n",
      "|    total_trades       | 1389      |\n",
      "| time/                 |           |\n",
      "|    fps                | 335       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.74     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 0.000469  |\n",
      "|    std                | 1.38      |\n",
      "|    value_loss         | 2e-07     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.00446  |\n",
      "|    std                | 1.41     |\n",
      "|    value_loss         | 3.73e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 338       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.78     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -0.000961 |\n",
      "|    std                | 1.43      |\n",
      "|    value_loss         | 5.15e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 338      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.8     |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.00226  |\n",
      "|    std                | 1.46     |\n",
      "|    value_loss         | 2.25e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 339      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 58       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.000985 |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 4.29e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.83e+04  |\n",
      "|    total_cost         | 1.5e+03   |\n",
      "|    total_reward       | -1.68e+03 |\n",
      "|    total_reward_pct   | -1.68     |\n",
      "|    total_trades       | 1223      |\n",
      "| time/                 |           |\n",
      "|    fps                | 340       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -0.00212  |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 1.22e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 342      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 0.00288  |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 1.61e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 343      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.85    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | 0.00224  |\n",
      "|    std                | 1.54     |\n",
      "|    value_loss         | 2.24e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 343      |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 64       |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.87    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 0.000269 |\n",
      "|    std                | 1.57     |\n",
      "|    value_loss         | 2.45e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 343      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 65       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 0.000886 |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 1.12e-07 |\n",
      "------------------------------------\n",
      "day: 2515, episode: 10\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 98494.37\n",
      "total_reward: -1505.63\n",
      "total_cost: 1601.95\n",
      "total_trades: 1189\n",
      "Sharpe: -0.314\n",
      "=================================\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.85e+04  |\n",
      "|    total_cost         | 1.6e+03   |\n",
      "|    total_reward       | -1.51e+03 |\n",
      "|    total_reward_pct   | -1.51     |\n",
      "|    total_trades       | 1189      |\n",
      "| time/                 |           |\n",
      "|    fps                | 344       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.89     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 0.00148   |\n",
      "|    std                | 1.61      |\n",
      "|    value_loss         | 1.27e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 345      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.91    |\n",
      "|    explained_variance | -7.2e+11 |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -0.00906 |\n",
      "|    std                | 1.63     |\n",
      "|    value_loss         | 2.65e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 346      |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.00773  |\n",
      "|    std                | 1.66     |\n",
      "|    value_loss         | 1.08e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 347      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.94    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 0.000802 |\n",
      "|    std                | 1.69     |\n",
      "|    value_loss         | 2.53e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 348      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.96    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 0.0297   |\n",
      "|    std                | 1.72     |\n",
      "|    value_loss         | 0.000278 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.97e+04  |\n",
      "|    total_cost         | 1.62e+03  |\n",
      "|    total_reward       | -296      |\n",
      "|    total_reward_pct   | -0.296    |\n",
      "|    total_trades       | 1198      |\n",
      "| time/                 |           |\n",
      "|    fps                | 350       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.97     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | -0.000358 |\n",
      "|    std                | 1.74      |\n",
      "|    value_loss         | 6.02e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 351      |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -0.00133 |\n",
      "|    std                | 1.77     |\n",
      "|    value_loss         | 5.52e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 352      |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 0.000887 |\n",
      "|    std                | 1.8      |\n",
      "|    value_loss         | 3.73e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 353      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 76       |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.03    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | -0.00551 |\n",
      "|    std                | 1.83     |\n",
      "|    value_loss         | 9.48e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 354      |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 77       |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.04    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.0054   |\n",
      "|    std                | 1.87     |\n",
      "|    value_loss         | 1e-05    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.37e+04  |\n",
      "|    total_cost         | 1.53e+03  |\n",
      "|    total_reward       | -6.27e+03 |\n",
      "|    total_reward_pct   | -6.27     |\n",
      "|    total_trades       | 1173      |\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 0.000604  |\n",
      "|    std                | 1.89      |\n",
      "|    value_loss         | 1.14e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 355      |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.07    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -0.00335 |\n",
      "|    std                | 1.92     |\n",
      "|    value_loss         | 2.52e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 356      |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 81       |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.09    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -0.0015  |\n",
      "|    std                | 1.96     |\n",
      "|    value_loss         | 5.73e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 357      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 82       |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -0.00293 |\n",
      "|    std                | 2        |\n",
      "|    value_loss         | 2.87e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 356       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -0.000805 |\n",
      "|    std                | 2.04      |\n",
      "|    value_loss         | 3.13e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.99e+04  |\n",
      "|    total_cost         | 1.61e+03  |\n",
      "|    total_reward       | -130      |\n",
      "|    total_reward_pct   | -0.13     |\n",
      "|    total_trades       | 1085      |\n",
      "| time/                 |           |\n",
      "|    fps                | 357       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.14     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -2.29e-05 |\n",
      "|    std                | 2.05      |\n",
      "|    value_loss         | 8e-10     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 356       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.15     |\n",
      "|    explained_variance | -3.25e+12 |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 0.000538  |\n",
      "|    std                | 2.07      |\n",
      "|    value_loss         | 9.46e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 357      |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 0.000337 |\n",
      "|    std                | 2.09     |\n",
      "|    value_loss         | 3.42e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 356      |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 0.0518   |\n",
      "|    std                | 2.13     |\n",
      "|    value_loss         | 0.00059  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 355      |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 91       |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.19    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 0.0109   |\n",
      "|    std                | 2.16     |\n",
      "|    value_loss         | 3.62e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.36e+04  |\n",
      "|    total_cost         | 1.78e+03  |\n",
      "|    total_reward       | -6.44e+03 |\n",
      "|    total_reward_pct   | -6.44     |\n",
      "|    total_trades       | 1097      |\n",
      "| time/                 |           |\n",
      "|    fps                | 356       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | -0.00115  |\n",
      "|    std                | 2.19      |\n",
      "|    value_loss         | 3.46e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 357      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 93       |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.22    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 0.000163 |\n",
      "|    std                | 2.23     |\n",
      "|    value_loss         | 3.23e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 357       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.24     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -0.000973 |\n",
      "|    std                | 2.27      |\n",
      "|    value_loss         | 3.08e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 358      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 96       |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.26    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | 0.0093   |\n",
      "|    std                | 2.31     |\n",
      "|    value_loss         | 2.15e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 359      |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | -0.00804 |\n",
      "|    std                | 2.34     |\n",
      "|    value_loss         | 1.57e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.85e+04  |\n",
      "|    total_cost         | 1.61e+03  |\n",
      "|    total_reward       | -1.53e+03 |\n",
      "|    total_reward_pct   | -1.53     |\n",
      "|    total_trades       | 1117      |\n",
      "| time/                 |           |\n",
      "|    fps                | 360       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -0.000552 |\n",
      "|    std                | 2.37      |\n",
      "|    value_loss         | 8.46e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 360       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.3      |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -0.000837 |\n",
      "|    std                | 2.42      |\n",
      "|    value_loss         | 2.25e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 361       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.32     |\n",
      "|    explained_variance | -6.87e+10 |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -0.00104  |\n",
      "|    std                | 2.46      |\n",
      "|    value_loss         | 5.14e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 362      |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.34    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 0.000351 |\n",
      "|    std                | 2.51     |\n",
      "|    value_loss         | 3.73e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 362       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.36     |\n",
      "|    explained_variance | -5.57e+11 |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -0.00692  |\n",
      "|    std                | 2.55      |\n",
      "|    value_loss         | 7.06e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| environment/          |          |\n",
      "|    portfolio_value    | 1e+05    |\n",
      "|    total_cost         | 1.48e+03 |\n",
      "|    total_reward       | 198      |\n",
      "|    total_reward_pct   | 0.198    |\n",
      "|    total_trades       | 996      |\n",
      "| time/                 |          |\n",
      "|    fps                | 363      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 104      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.37    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 0.000373 |\n",
      "|    std                | 2.59     |\n",
      "|    value_loss         | 3.46e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 363       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.39     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -0.000824 |\n",
      "|    std                | 2.63      |\n",
      "|    value_loss         | 1.73e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 363      |\n",
      "|    iterations         | 7800     |\n",
      "|    time_elapsed       | 107      |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.4     |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 7799     |\n",
      "|    policy_loss        | -0.00232 |\n",
      "|    std                | 2.67     |\n",
      "|    value_loss         | 7.87e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 364       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.42     |\n",
      "|    explained_variance | -4.53e+10 |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 0.00391   |\n",
      "|    std                | 2.72      |\n",
      "|    value_loss         | 3.83e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 365      |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.44    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | 0.000605 |\n",
      "|    std                | 2.76     |\n",
      "|    value_loss         | 5.83e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.46e+04  |\n",
      "|    total_cost         | 1.83e+03  |\n",
      "|    total_reward       | -5.37e+03 |\n",
      "|    total_reward_pct   | -5.37     |\n",
      "|    total_trades       | 1123      |\n",
      "| time/                 |           |\n",
      "|    fps                | 365       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.44     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 0.00827   |\n",
      "|    std                | 2.79      |\n",
      "|    value_loss         | 1.55e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 366      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.45    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | 0.000668 |\n",
      "|    std                | 2.81     |\n",
      "|    value_loss         | 9.61e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 366      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 113      |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.46    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | 0.00129  |\n",
      "|    std                | 2.83     |\n",
      "|    value_loss         | 3.12e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 367      |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.47    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | 0.00324  |\n",
      "|    std                | 2.87     |\n",
      "|    value_loss         | 2.22e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 367      |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 115      |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.49    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | -0.00595 |\n",
      "|    std                | 2.91     |\n",
      "|    value_loss         | 7.85e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| environment/          |          |\n",
      "|    portfolio_value    | 9.48e+04 |\n",
      "|    total_cost         | 1.54e+03 |\n",
      "|    total_reward       | -5.2e+03 |\n",
      "|    total_reward_pct   | -5.2     |\n",
      "|    total_trades       | 1002     |\n",
      "| time/                 |          |\n",
      "|    fps                | 367      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 116      |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.51    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | 0.00102  |\n",
      "|    std                | 2.96     |\n",
      "|    value_loss         | 6.37e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 368      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 118      |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.52    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | 0.000842 |\n",
      "|    std                | 3.02     |\n",
      "|    value_loss         | 4e-07    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 368       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -3.71e-05 |\n",
      "|    std                | 3.08      |\n",
      "|    value_loss         | 3.68e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 369      |\n",
      "|    iterations         | 8900     |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.56    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 8899     |\n",
      "|    policy_loss        | -0.0182  |\n",
      "|    std                | 3.14     |\n",
      "|    value_loss         | 5.51e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 369      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.58    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 0.0984   |\n",
      "|    std                | 3.2      |\n",
      "|    value_loss         | 0.00139  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| environment/          |          |\n",
      "|    portfolio_value    | 1e+05    |\n",
      "|    total_cost         | 1.94e+03 |\n",
      "|    total_reward       | 309      |\n",
      "|    total_reward_pct   | 0.309    |\n",
      "|    total_trades       | 1127     |\n",
      "| time/                 |          |\n",
      "|    fps                | 370      |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 122      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.59    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | -0.00179 |\n",
      "|    std                | 3.23     |\n",
      "|    value_loss         | 5.39e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 370      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 124      |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.61    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -0.00514 |\n",
      "|    std                | 3.28     |\n",
      "|    value_loss         | 3.78e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 371      |\n",
      "|    iterations         | 9300     |\n",
      "|    time_elapsed       | 125      |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.62    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 9299     |\n",
      "|    policy_loss        | 0.00439  |\n",
      "|    std                | 3.34     |\n",
      "|    value_loss         | 3.82e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 371      |\n",
      "|    iterations         | 9400     |\n",
      "|    time_elapsed       | 126      |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.64    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 9399     |\n",
      "|    policy_loss        | 0.00103  |\n",
      "|    std                | 3.4      |\n",
      "|    value_loss         | 2.6e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.66     |\n",
      "|    explained_variance | -1.82e+10 |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | 0.00185   |\n",
      "|    std                | 3.44      |\n",
      "|    value_loss         | 7.21e-07  |\n",
      "-------------------------------------\n",
      "day: 2515, episode: 20\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 98469.76\n",
      "total_reward: -1530.24\n",
      "total_cost: 2022.80\n",
      "total_trades: 1231\n",
      "Sharpe: -0.400\n",
      "=================================\n",
      "-------------------------------------\n",
      "| environment/          |           |\n",
      "|    portfolio_value    | 9.85e+04  |\n",
      "|    total_cost         | 2.02e+03  |\n",
      "|    total_reward       | -1.53e+03 |\n",
      "|    total_reward_pct   | -1.53     |\n",
      "|    total_trades       | 1231      |\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.67     |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | -0.00078  |\n",
      "|    std                | 3.49      |\n",
      "|    value_loss         | 6.35e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 372      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.69    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | -0.00709 |\n",
      "|    std                | 3.55     |\n",
      "|    value_loss         | 4.93e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.7     |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | 0.033    |\n",
      "|    std                | 3.62     |\n",
      "|    value_loss         | 0.000248 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 373      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 132      |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.72    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0002   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | -0.0181  |\n",
      "|    std                | 3.67     |\n",
      "|    value_loss         | 4.29e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 373       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.74     |\n",
      "|    explained_variance | -2.03e+11 |\n",
      "|    learning_rate      | 0.0002    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -0.00244  |\n",
      "|    std                | 3.73      |\n",
      "|    value_loss         | 1.19e-06  |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                                tb_log_name='a2c',\n",
    "                                total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9upN8FI2r_X1"
   },
   "source": [
    "### Model 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUFJlHsi-Ka-",
    "outputId": "1b7db70f-f740-401c-ab55-a567b4235e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'buffer_size': 500000, 'learning_rate': 0.0001}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "DDPG_PARAMS = {\"batch_size\": 64, \"buffer_size\": 500000, \"learning_rate\": 0.0001}\n",
    "\n",
    "\n",
    "model_ddpg = agent.get_model(\"ddpg\",model_kwargs = DDPG_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5J_Scwp-Nis",
    "outputId": "a538962d-33c8-44d3-94af-6ef61fc0d230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| environment/          |          |\n",
      "|    portfolio_value    | 8.6e+05  |\n",
      "|    total_cost         | 99.9     |\n",
      "|    total_reward       | 7.6e+05  |\n",
      "|    total_reward_pct   | 760      |\n",
      "|    total_trades       | 2515     |\n",
      "| time/                 |          |\n",
      "|    episodes           | 4        |\n",
      "|    fps                | 97       |\n",
      "|    time_elapsed       | 103      |\n",
      "|    total timesteps    | 10064    |\n",
      "| train/                |          |\n",
      "|    actor_loss         | -158     |\n",
      "|    critic_loss        | 407      |\n",
      "|    entropy_loss       | -2.74    |\n",
      "|    explained_variance | nan      |\n",
      "|    learning_rate      | 0.0001   |\n",
      "|    n_updates          | 7548     |\n",
      "|    policy_loss        | -0.00302 |\n",
      "|    std                | 3.73     |\n",
      "|    value_loss         | 3.81e-06 |\n",
      "------------------------------------\n",
      "----------------------------------\n",
      "| environment/        |          |\n",
      "|    portfolio_value  | 8.6e+05  |\n",
      "|    total_cost       | 99.9     |\n",
      "|    total_reward     | 7.6e+05  |\n",
      "|    total_reward_pct | 760      |\n",
      "|    total_trades     | 2515     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 84       |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total timesteps  | 20128    |\n",
      "| train/              |          |\n",
      "|    actor_loss       | -112     |\n",
      "|    critic_loss      | 214      |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    n_updates        | 17612    |\n",
      "----------------------------------\n",
      "day: 2515, episode: 30\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 860124.61\n",
      "total_reward: 760124.61\n",
      "total_cost: 99.90\n",
      "total_trades: 2515\n",
      "Sharpe: 0.990\n",
      "=================================\n",
      "----------------------------------\n",
      "| environment/        |          |\n",
      "|    portfolio_value  | 8.6e+05  |\n",
      "|    total_cost       | 99.9     |\n",
      "|    total_reward     | 7.6e+05  |\n",
      "|    total_reward_pct | 760      |\n",
      "|    total_trades     | 2515     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 76       |\n",
      "|    time_elapsed     | 396      |\n",
      "|    total timesteps  | 30192    |\n",
      "| train/              |          |\n",
      "|    actor_loss       | -75.2    |\n",
      "|    critic_loss      | 138      |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    n_updates        | 27676    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sve9WGvsC__"
   },
   "source": [
    "### Model 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOjuycpS-Qvn",
    "outputId": "13ab0019-419b-4e6e-949d-c2617ae296b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 128}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.005,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G9hCHA8A-RSy",
    "outputId": "df865883-f3a8-42b2-951f-b129b44bd248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    fps             | 628      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69      |\n",
      "|    critic_loss     | 90.3     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 30192    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1.16e+05     |\n",
      "|    total_cost           | 3.21e+03     |\n",
      "|    total_reward         | 1.63e+04     |\n",
      "|    total_reward_pct     | 16.3         |\n",
      "|    total_trades         | 2442         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 575          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040650377 |\n",
      "|    clip_fraction        | 0.004        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | nan          |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.00732     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000624    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00502      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 2.39e+05     |\n",
      "|    total_cost           | 3.39e+03     |\n",
      "|    total_reward         | 1.39e+05     |\n",
      "|    total_reward_pct     | 139          |\n",
      "|    total_trades         | 2501         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 558          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008233246 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.316       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.092        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000357    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.118        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 1.05e+05      |\n",
      "|    total_cost           | 3.26e+03      |\n",
      "|    total_reward         | 4.71e+03      |\n",
      "|    total_reward_pct     | 4.71          |\n",
      "|    total_trades         | 2447          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 552           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 14            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -5.599833e-05 |\n",
      "|    clip_fraction        | 0.00718       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -4.43         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 0.134         |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00137      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.312         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 1.16e+05      |\n",
      "|    total_cost           | 3.02e+03      |\n",
      "|    total_reward         | 1.55e+04      |\n",
      "|    total_reward_pct     | 15.5          |\n",
      "|    total_trades         | 2387          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 547           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 18            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015230017 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -3.08e+13     |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | -0.00203      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -2.55e-05     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.0069        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043210494 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -4.05e+12    |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | -0.013       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.00363      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 1.94e+05      |\n",
      "|    total_cost           | 3.36e+03      |\n",
      "|    total_reward         | 9.36e+04      |\n",
      "|    total_reward_pct     | 93.6          |\n",
      "|    total_trades         | 2488          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 541           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044591166 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -45.5         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 0.0489        |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.00111      |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 0.0832        |\n",
      "-------------------------------------------\n",
      "day: 2515, episode: 40\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 121642.30\n",
      "total_reward: 21642.30\n",
      "total_cost: 3277.04\n",
      "total_trades: 2467\n",
      "Sharpe: 0.289\n",
      "=================================\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1.22e+05     |\n",
      "|    total_cost           | 3.28e+03     |\n",
      "|    total_reward         | 2.16e+04     |\n",
      "|    total_reward_pct     | 21.6         |\n",
      "|    total_trades         | 2467         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016458448 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -61.9        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.00811      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -3.55e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.133        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1.25e+05     |\n",
      "|    total_cost           | 3.29e+03     |\n",
      "|    total_reward         | 2.5e+04      |\n",
      "|    total_reward_pct     | 25           |\n",
      "|    total_trades         | 2465         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026776893 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.56e+14    |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0246       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000337    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.0348       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 2.26e+05    |\n",
      "|    total_cost           | 3.27e+03    |\n",
      "|    total_reward         | 1.26e+05    |\n",
      "|    total_reward_pct     | 126         |\n",
      "|    total_trades         | 2484        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 508         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004694904 |\n",
      "|    clip_fraction        | 0.0135      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -27.9       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00472    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.0545      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 504           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 44            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | -0.0001871738 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -14.4         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 0.155         |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.000864     |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 0.417         |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 2.93e+05    |\n",
      "|    total_cost           | 3.36e+03    |\n",
      "|    total_reward         | 1.93e+05    |\n",
      "|    total_reward_pct     | 193         |\n",
      "|    total_trades         | 2494        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 498         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002075892 |\n",
      "|    clip_fraction        | 0.00156     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -30.8       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.207       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.000405   |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.352       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 3.42e+05     |\n",
      "|    total_cost           | 3.35e+03     |\n",
      "|    total_reward         | 2.42e+05     |\n",
      "|    total_reward_pct     | 242          |\n",
      "|    total_trades         | 2493         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 500          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021867384 |\n",
      "|    clip_fraction        | 0.00728      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -13.6        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.283        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.689        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 2.88e+05     |\n",
      "|    total_cost           | 3.39e+03     |\n",
      "|    total_reward         | 1.88e+05     |\n",
      "|    total_reward_pct     | 188          |\n",
      "|    total_trades         | 2490         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017796736 |\n",
      "|    clip_fraction        | 0.00107      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -20.5        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.343        |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000257    |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.793        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 3.57e+05     |\n",
      "|    total_cost           | 3.36e+03     |\n",
      "|    total_reward         | 2.57e+05     |\n",
      "|    total_reward_pct     | 257          |\n",
      "|    total_trades         | 2493         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020431618 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -5.4         |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.214        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000914    |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 0.637        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 3.75e+05    |\n",
      "|    total_cost           | 3.27e+03    |\n",
      "|    total_reward         | 2.75e+05    |\n",
      "|    total_reward_pct     | 275         |\n",
      "|    total_trades         | 2486        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 504         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004443202 |\n",
      "|    clip_fraction        | 0.015       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -5.73       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.606       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00207    |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 1.04        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 505           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 68            |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00064537954 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -13.3         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 0.575         |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -6.21e-05     |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 1.57          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 5.01e+05     |\n",
      "|    total_cost           | 3.32e+03     |\n",
      "|    total_reward         | 4.01e+05     |\n",
      "|    total_reward_pct     | 401          |\n",
      "|    total_trades         | 2488         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 506          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002467821 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -9.62        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.751        |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000705    |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 1.2          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 4.71e+05     |\n",
      "|    total_cost           | 3.27e+03     |\n",
      "|    total_reward         | 3.71e+05     |\n",
      "|    total_reward_pct     | 371          |\n",
      "|    total_trades         | 2493         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 507          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023473827 |\n",
      "|    clip_fraction        | 0.00615      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -13.8        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.889        |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 2.23         |\n",
      "------------------------------------------\n",
      "day: 2515, episode: 50\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 427387.15\n",
      "total_reward: 327387.15\n",
      "total_cost: 3384.72\n",
      "total_trades: 2487\n",
      "Sharpe: 0.803\n",
      "=================================\n",
      "----------------------------------------\n",
      "| environment/            |            |\n",
      "|    portfolio_value      | 4.27e+05   |\n",
      "|    total_cost           | 3.38e+03   |\n",
      "|    total_reward         | 3.27e+05   |\n",
      "|    total_reward_pct     | 327        |\n",
      "|    total_trades         | 2487       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 508        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 80         |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00251247 |\n",
      "|    clip_fraction        | 0.00747    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | -11.4      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.732      |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.000988  |\n",
      "|    std                  | 0.994      |\n",
      "|    value_loss           | 2.11       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 4.17e+05     |\n",
      "|    total_cost           | 3.26e+03     |\n",
      "|    total_reward         | 3.17e+05     |\n",
      "|    total_reward_pct     | 317          |\n",
      "|    total_trades         | 2491         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 507          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042447248 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -9.78        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.42         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000775    |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 1.74         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 506          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022075712 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -10.4        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.16         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 2            |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 5.07e+05     |\n",
      "|    total_cost           | 3.19e+03     |\n",
      "|    total_reward         | 4.07e+05     |\n",
      "|    total_reward_pct     | 407          |\n",
      "|    total_trades         | 2492         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 507          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017876056 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -16.7        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.517        |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 1.38         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 6.05e+05    |\n",
      "|    total_cost           | 3.29e+03    |\n",
      "|    total_reward         | 5.05e+05    |\n",
      "|    total_reward_pct     | 505         |\n",
      "|    total_trades         | 2494        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 504         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000654249 |\n",
      "|    clip_fraction        | 0.00815     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -23.4       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.33        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00183    |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 2.86        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 5.81e+05     |\n",
      "|    total_cost           | 3.14e+03     |\n",
      "|    total_reward         | 4.81e+05     |\n",
      "|    total_reward_pct     | 481          |\n",
      "|    total_trades         | 2496         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 505          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014609965 |\n",
      "|    clip_fraction        | 0.00303      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -28.2        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.3          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000655    |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 3.67         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 5.78e+05     |\n",
      "|    total_cost           | 3.27e+03     |\n",
      "|    total_reward         | 4.78e+05     |\n",
      "|    total_reward_pct     | 478          |\n",
      "|    total_trades         | 2505         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022151617 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -22.8        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.87         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000745    |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 3.61         |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 501            |\n",
      "|    iterations           | 27             |\n",
      "|    time_elapsed         | 110            |\n",
      "|    total_timesteps      | 55296          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | -2.2943132e-06 |\n",
      "|    clip_fraction        | 4.88e-05       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.4           |\n",
      "|    explained_variance   | -19.4          |\n",
      "|    learning_rate        | 0.0001         |\n",
      "|    loss                 | 1.37           |\n",
      "|    n_updates            | 260            |\n",
      "|    policy_gradient_loss | -0.0004        |\n",
      "|    std                  | 0.986          |\n",
      "|    value_loss           | 3.74           |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 6.01e+05     |\n",
      "|    total_cost           | 3.15e+03     |\n",
      "|    total_reward         | 5.01e+05     |\n",
      "|    total_reward_pct     | 501          |\n",
      "|    total_trades         | 2487         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 502          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051125595 |\n",
      "|    clip_fraction        | 0.00122      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -100         |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.11         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000504    |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 2.78         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 5.83e+05      |\n",
      "|    total_cost           | 3.29e+03      |\n",
      "|    total_reward         | 4.83e+05      |\n",
      "|    total_reward_pct     | 483           |\n",
      "|    total_trades         | 2496          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 502           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 118           |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020267363 |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -23.1         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 1.58          |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.000472     |\n",
      "|    std                  | 0.99          |\n",
      "|    value_loss           | 2.65          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 5.87e+05     |\n",
      "|    total_cost           | 3.24e+03     |\n",
      "|    total_reward         | 4.87e+05     |\n",
      "|    total_reward_pct     | 487          |\n",
      "|    total_trades         | 2495         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 503          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018342568 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -40.1        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.693        |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.000757    |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 3.33         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 5.7e+05     |\n",
      "|    total_cost           | 3.24e+03    |\n",
      "|    total_reward         | 4.7e+05     |\n",
      "|    total_reward_pct     | 470         |\n",
      "|    total_trades         | 2498        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 125         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002805877 |\n",
      "|    clip_fraction        | 0.00615     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -26.7       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 3.78        |\n",
      "-----------------------------------------\n",
      "day: 2515, episode: 60\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 584766.04\n",
      "total_reward: 484766.04\n",
      "total_cost: 3288.07\n",
      "total_trades: 2492\n",
      "Sharpe: 0.899\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 5.85e+05      |\n",
      "|    total_cost           | 3.29e+03      |\n",
      "|    total_reward         | 4.85e+05      |\n",
      "|    total_reward_pct     | 485           |\n",
      "|    total_trades         | 2492          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 504           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 129           |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2817923e-06 |\n",
      "|    clip_fraction        | 0.00137       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -34.8         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 1.77          |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000942     |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 3.42          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 505          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 133          |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028439905 |\n",
      "|    clip_fraction        | 0.00229      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -42.1        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.12         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 3.99         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| environment/            |               |\n",
      "|    portfolio_value      | 5.42e+05      |\n",
      "|    total_cost           | 3.31e+03      |\n",
      "|    total_reward         | 4.42e+05      |\n",
      "|    total_reward_pct     | 442           |\n",
      "|    total_trades         | 2504          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 505           |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 137           |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056815404 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -19.3         |\n",
      "|    learning_rate        | 0.0001        |\n",
      "|    loss                 | 0.692         |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.00133      |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 1.45          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 5.79e+05     |\n",
      "|    total_cost           | 3.3e+03      |\n",
      "|    total_reward         | 4.79e+05     |\n",
      "|    total_reward_pct     | 479          |\n",
      "|    total_trades         | 2495         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 506          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 141          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044840425 |\n",
      "|    clip_fraction        | 0.00244      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -20.3        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.000604    |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 2.95         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 5.92e+05     |\n",
      "|    total_cost           | 3.38e+03     |\n",
      "|    total_reward         | 4.92e+05     |\n",
      "|    total_reward_pct     | 492          |\n",
      "|    total_trades         | 2490         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 507          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010413921 |\n",
      "|    clip_fraction        | 0.00142      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -27          |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 2.51         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000607    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 3.53         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| environment/            |             |\n",
      "|    portfolio_value      | 6.14e+05    |\n",
      "|    total_cost           | 3.4e+03     |\n",
      "|    total_reward         | 5.14e+05    |\n",
      "|    total_reward_pct     | 514         |\n",
      "|    total_trades         | 2495        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 505         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010507213 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -29.3       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00804    |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 505         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 153         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002468641 |\n",
      "|    clip_fraction        | 0.0146      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -13.8       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 2.22        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.000564   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| environment/            |            |\n",
      "|    portfolio_value      | 6.24e+05   |\n",
      "|    total_cost           | 3.39e+03   |\n",
      "|    total_reward         | 5.24e+05   |\n",
      "|    total_reward_pct     | 524        |\n",
      "|    total_trades         | 2497       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 505        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 158        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00159422 |\n",
      "|    clip_fraction        | 0.0111     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | -125       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 1.02       |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.000171  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 2.33       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 6.14e+05     |\n",
      "|    total_cost           | 3.45e+03     |\n",
      "|    total_reward         | 5.14e+05     |\n",
      "|    total_reward_pct     | 514          |\n",
      "|    total_trades         | 2503         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 505          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 162          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034753499 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -21.6        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 1.51         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.6          |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=80000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiBG2ZknsG73"
   },
   "source": [
    "### Model 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcRdoBc8-Xze",
    "outputId": "cc0a65bf-f79d-48ce-c173-6ae08747b0fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 1000000, 'learning_rate': 0.0003}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 128, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.0003}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CB5cAAio-ZSs",
    "outputId": "741a1ed9-9bb0-4763-f237-5cd461ac100e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2515, episode: 70\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 100000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "------------------------------------------\n",
      "| environment/            |              |\n",
      "|    portfolio_value      | 1e+05        |\n",
      "|    total_cost           | 0            |\n",
      "|    total_reward         | 0            |\n",
      "|    total_reward_pct     | 0            |\n",
      "|    total_trades         | 0            |\n",
      "| time/                   |              |\n",
      "|    episodes             | 4            |\n",
      "|    fps                  | 70           |\n",
      "|    time_elapsed         | 143          |\n",
      "|    total timesteps      | 10064        |\n",
      "| train/                  |              |\n",
      "|    actor_loss           | 1.8e+03      |\n",
      "|    approx_kl            | 0.0051646736 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    critic_loss          | 9.86e+03     |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -36.7        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43         |\n",
      "|    n_updates            | 7548         |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.62         |\n",
      "------------------------------------------\n",
      "----------------------------------\n",
      "| environment/        |          |\n",
      "|    portfolio_value  | 1e+05    |\n",
      "|    total_cost       | 0        |\n",
      "|    total_reward     | 0        |\n",
      "|    total_reward_pct | 0        |\n",
      "|    total_trades     | 0        |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 346      |\n",
      "|    total timesteps  | 20128    |\n",
      "| train/              |          |\n",
      "|    actor_loss       | 1.4e+03  |\n",
      "|    critic_loss      | 3.33e+03 |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    n_updates        | 17612    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| environment/        |          |\n",
      "|    portfolio_value  | 1e+05    |\n",
      "|    total_cost       | 0        |\n",
      "|    total_reward     | 0        |\n",
      "|    total_reward_pct | 0        |\n",
      "|    total_trades     | 0        |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 533      |\n",
      "|    total timesteps  | 30192    |\n",
      "| train/              |          |\n",
      "|    actor_loss       | 1.08e+03 |\n",
      "|    critic_loss      | 1.34e+03 |\n",
      "|    learning_rate    | 0.0003   |\n",
      "|    n_updates        | 27676    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDk2qrlTLZCp"
   },
   "source": [
    "### Model 4: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "el8sK4fo-dl1",
    "outputId": "20c5793c-397b-4dc3-a702-857178e61813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 3e-05, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.00003,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gaF80PR0-d_d",
    "outputId": "b26b6435-ca69-4000-b2b3-22596677d8ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| environment/        |           |\n",
      "|    portfolio_value  | 1e+05     |\n",
      "|    total_cost       | 0         |\n",
      "|    total_reward     | 0         |\n",
      "|    total_reward_pct | 0         |\n",
      "|    total_trades     | 0         |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 50        |\n",
      "|    time_elapsed     | 199       |\n",
      "|    total timesteps  | 10064     |\n",
      "| train/              |           |\n",
      "|    actor_loss       | -1.06e+03 |\n",
      "|    critic_loss      | 3.5       |\n",
      "|    ent_coef         | 0.135     |\n",
      "|    ent_coef_loss    | 63.9      |\n",
      "|    learning_rate    | 3e-05     |\n",
      "|    n_updates        | 9963      |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| environment/        |          |\n",
      "|    portfolio_value  | 1e+05    |\n",
      "|    total_cost       | 0        |\n",
      "|    total_reward     | 0        |\n",
      "|    total_reward_pct | 0        |\n",
      "|    total_trades     | 0        |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 50       |\n",
      "|    time_elapsed     | 401      |\n",
      "|    total timesteps  | 20128    |\n",
      "| train/              |          |\n",
      "|    actor_loss       | -388     |\n",
      "|    critic_loss      | 2.28     |\n",
      "|    ent_coef         | 0.183    |\n",
      "|    ent_coef_loss    | 54.3     |\n",
      "|    learning_rate    | 3e-05    |\n",
      "|    n_updates        | 20027    |\n",
      "----------------------------------\n",
      "day: 2515, episode: 90\n",
      "begin_total_asset: 100000.00\n",
      "end_total_asset: 100000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KE1Xm9N9TnTn"
   },
   "source": [
    "### Trading\n",
    "* we use the environment class we initialized at 5.3 to create a stock trading environment\n",
    "* Assume that we have $100,000 initial capital at 2019-01-01. \n",
    "* We use the trained model of PPO to trade AAPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "kLFUwxhCoHN_",
    "outputId": "22540921-fc5e-460b-f9ef-1438131b45d7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>...</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>kdjk</th>\n",
       "      <th>open_2_sma</th>\n",
       "      <th>boll</th>\n",
       "      <th>close_10.0_le_5_c</th>\n",
       "      <th>wr_10</th>\n",
       "      <th>dma</th>\n",
       "      <th>trix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>38.722500</td>\n",
       "      <td>39.712502</td>\n",
       "      <td>38.557499</td>\n",
       "      <td>38.562561</td>\n",
       "      <td>148158800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.019903</td>\n",
       "      <td>44.572026</td>\n",
       "      <td>...</td>\n",
       "      <td>42.250808</td>\n",
       "      <td>41.287324</td>\n",
       "      <td>46.557656</td>\n",
       "      <td>27.327775</td>\n",
       "      <td>39.177500</td>\n",
       "      <td>40.034790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63.418114</td>\n",
       "      <td>-6.886014</td>\n",
       "      <td>-0.761652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>35.994999</td>\n",
       "      <td>36.430000</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>34.721451</td>\n",
       "      <td>365248800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.203028</td>\n",
       "      <td>43.977597</td>\n",
       "      <td>...</td>\n",
       "      <td>55.246973</td>\n",
       "      <td>40.869433</td>\n",
       "      <td>46.226696</td>\n",
       "      <td>13.056580</td>\n",
       "      <td>37.358749</td>\n",
       "      <td>39.514298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.236531</td>\n",
       "      <td>-7.096225</td>\n",
       "      <td>-0.763466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>36.132500</td>\n",
       "      <td>37.137501</td>\n",
       "      <td>35.950001</td>\n",
       "      <td>36.203678</td>\n",
       "      <td>234428400</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4</td>\n",
       "      <td>-2.203157</td>\n",
       "      <td>43.519698</td>\n",
       "      <td>...</td>\n",
       "      <td>47.060632</td>\n",
       "      <td>40.563381</td>\n",
       "      <td>45.922549</td>\n",
       "      <td>14.108980</td>\n",
       "      <td>36.063749</td>\n",
       "      <td>39.167181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.003419</td>\n",
       "      <td>-7.054847</td>\n",
       "      <td>-0.766086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>37.174999</td>\n",
       "      <td>37.207500</td>\n",
       "      <td>36.474998</td>\n",
       "      <td>36.123104</td>\n",
       "      <td>219111200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.184578</td>\n",
       "      <td>43.067268</td>\n",
       "      <td>...</td>\n",
       "      <td>46.245025</td>\n",
       "      <td>40.326923</td>\n",
       "      <td>45.604486</td>\n",
       "      <td>14.191732</td>\n",
       "      <td>36.653749</td>\n",
       "      <td>38.840088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.642763</td>\n",
       "      <td>-6.910649</td>\n",
       "      <td>-0.767321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-08</td>\n",
       "      <td>37.389999</td>\n",
       "      <td>37.955002</td>\n",
       "      <td>37.130001</td>\n",
       "      <td>36.811718</td>\n",
       "      <td>164101200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.090194</td>\n",
       "      <td>42.797282</td>\n",
       "      <td>...</td>\n",
       "      <td>37.537680</td>\n",
       "      <td>40.115047</td>\n",
       "      <td>45.340525</td>\n",
       "      <td>19.535793</td>\n",
       "      <td>37.282499</td>\n",
       "      <td>38.623490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.776084</td>\n",
       "      <td>-6.599589</td>\n",
       "      <td>-0.759067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       open       high        low      close     volume   tic  \\\n",
       "0  2019-01-02  38.722500  39.712502  38.557499  38.562561  148158800  AAPL   \n",
       "1  2019-01-03  35.994999  36.430000  35.500000  34.721451  365248800  AAPL   \n",
       "2  2019-01-04  36.132500  37.137501  35.950001  36.203678  234428400  AAPL   \n",
       "3  2019-01-07  37.174999  37.207500  36.474998  36.123104  219111200  AAPL   \n",
       "4  2019-01-08  37.389999  37.955002  37.130001  36.811718  164101200  AAPL   \n",
       "\n",
       "   day      macd    boll_ub  ...      dx_30  close_30_sma  close_60_sma  \\\n",
       "0    2 -2.019903  44.572026  ...  42.250808     41.287324     46.557656   \n",
       "1    3 -2.203028  43.977597  ...  55.246973     40.869433     46.226696   \n",
       "2    4 -2.203157  43.519698  ...  47.060632     40.563381     45.922549   \n",
       "3    0 -2.184578  43.067268  ...  46.245025     40.326923     45.604486   \n",
       "4    1 -2.090194  42.797282  ...  37.537680     40.115047     45.340525   \n",
       "\n",
       "        kdjk  open_2_sma       boll  close_10.0_le_5_c       wr_10       dma  \\\n",
       "0  27.327775   39.177500  40.034790                0.0   63.418114 -6.886014   \n",
       "1  13.056580   37.358749  39.514298                0.0  112.236531 -7.096225   \n",
       "2  14.108980   36.063749  39.167181                0.0   86.003419 -7.054847   \n",
       "3  14.191732   36.653749  38.840088                0.0   85.642763 -6.910649   \n",
       "4  19.535793   37.282499  38.623490                0.0   69.776084 -6.599589   \n",
       "\n",
       "       trix  \n",
       "0 -0.761652  \n",
       "1 -0.763466  \n",
       "2 -0.766086  \n",
       "3 -0.767321  \n",
       "4 -0.759067  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bLHl6V7eqV6_"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DRL_prediction() got an unexpected keyword argument 'trade_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-52459e78ba62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                            \u001b[0mtrade_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtrade\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                            \u001b[0mtest_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_trade\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                            test_obs = obs_trade)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: DRL_prediction() got an unexpected keyword argument 'trade_data'"
     ]
    }
   ],
   "source": [
    "## make a prediction and get the account value change\n",
    "trade = data_split(data_df, start = '2019-01-01', end = '2021-01-01')\n",
    "\n",
    "e_trade_gym = StockTradingEnv(df = trade, **env_kwargs)\n",
    "env_trade, obs_trade = e_trade_gym.get_sb_env()\n",
    "\n",
    "df_account_value, df_actions = DRLAgent.DRL_prediction(model=trained_sac,\n",
    "                                           trade_data  = trade,\n",
    "                                           test_env = env_trade,\n",
    "                                           test_obs = obs_trade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYXxFzD5TnTw"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtesting Performance\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GwqOO-v1NVz"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C9cpPm9YYxHC",
    "outputId": "7c7a8e34-7ed3-441d-9036-829240430843"
   },
   "outputs": [],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = BackTestStats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
    "perf_stats_all.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-Tenjb0hcNr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4Gw3HNr1TDU"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MA_8LuZE1J3X",
    "outputId": "3d2c46b2-2b6e-4cfd-9648-68ae7d37d89c"
   },
   "outputs": [],
   "source": [
    "print(\"==============Compare to AAPL itself buy-and-hold===========\")\n",
    "%matplotlib inline\n",
    "BackTestPlot(account_value=df_account_value, \n",
    "             baseline_ticker = 'AAPL',\n",
    "             baseline_start = '2019-01-01',\n",
    "             baseline_end = '2021-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSFMdgCJE4O-"
   },
   "source": [
    "<a id='6.3'></a>\n",
    "## 7.3 Baseline Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MpR7aQIwqdC4",
    "outputId": "24029a71-3596-4090-d7bc-7f44dd6000fc"
   },
   "outputs": [],
   "source": [
    "print(\"==============Get Baseline Stats===========\")\n",
    "baesline_perf_stats=BaselineStats('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSFRXQfYFTQf",
    "outputId": "a5951bab-aa47-4fa8-9cd9-9fc62b5b02f5"
   },
   "outputs": [],
   "source": [
    "print(\"==============Get Baseline Stats===========\")\n",
    "baesline_perf_stats=BaselineStats('^GSPC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tunrA4mjE9la"
   },
   "source": [
    "<a id='6.4'></a>\n",
    "## 7.4 Compare to Stock Market Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5Ca2gHxi1gzX",
    "outputId": "68de7e8c-882a-4e8a-c9a9-cea8ea6ca212"
   },
   "outputs": [],
   "source": [
    "print(\"==============Compare to S&P 500===========\")\n",
    "%matplotlib inline\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "BackTestPlot(df_account_value, baseline_ticker = '^GSPC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_aHGEOTCluG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Y9CM5DeIr9GC",
    "9upN8FI2r_X1",
    "CiBG2ZknsG73"
   ],
   "include_colab_link": true,
   "name": "FinRL_single_stock_trading.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
